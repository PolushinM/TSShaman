### **AutoML библиотека для прогнозирования временных рядов «TSShaman»**

##### Ключевые особенности библиотеки:

- Максимально простой API, возможность использования модели через fit-predict, где в качестве параметров, в простейшем случае, требуется указать только период наблюдений, используемый для прогноза (окно) - для fit и период прогноза - для predict.
- Итоговая модель является ансамблем (бустингом) двух моделей: линейной регрессии (LR) и градиентного бустинга (GBMT), обучаемого на остатках линейной модели. Линейная модель (используется Scikit-learn SGDRegressor) выявляет основные авторегрессионные зависимости временного ряда, улучшает способность модели к интерполяции и экстраполяции. Градиентный бустинг (используется CatBoost ) выявляет нелинейные зависимости и позволяет учитывать категориальные признаки {TODO}.
- LR и GBMT обучаются на автоматически сгенерированных свойствах ряда (синтетических фичах). Свойства ряда генерируются и отбираются по специальному сложному, но универсальному  алгоритму, обеспечивающему баланс качества и скорости обучения модели.
- В других AutoML системах для временных рядов используются, как правило, существенно отличные архитектуры моделей, это означает, что стекинг TSShaman с другими моделями может показывать хорошие результаты.
- Использование экзогенных (вспомогательных) временных рядов, линейных фичей {TODO};
- Работа с временными рядами, имеющими пропуски {TODO};
- Возможность использования пользовательских фичей (линейных, нелинейных, категориальных)

##### Последовательность построения модели:

1. Автоматическая генерация свойств (синтетических фичей) ряда следующих видов:
   - Time-lag;
   - Time-lag фичи дифференцированного ряда (период дифференцирования и лаги определяются автоматически) {TODO};
   - Для Time-lag фичей производится обрезание выбросов за пределами интервала ±3std {**TODO**}.
   -  TimeData encoding – вектор состояния текущего времени, учитывающий время года, начало-конец года, новый год (смену года), день недели, выходные, время дня, используя линейные значения, функции синуса и косинуса. Длина вектора до 10 значений (все значения float, в будущем планируется добавление государственных праздников);
   -  Exponentially weighted moving average, Double, Triple и Quadriple exponential moving average (генерация большого количества EMA, DMA, TMA и QMA различных периодов на исходном и дифференцированном {TODO} рядах с последующей оценкой их значимости для LR и GBMT, предварительный выбор наиболее значимых и, одновременно, максимально отличающихся друг от друга окон скользящих средних, количество отобранных свойств зависит от общего гиперпараметра модели «Omega»);
   -  Для экзогенных рядов – генерация тех же фичей, но в меньшем количестве {TODO}. 
2. L1 feature selection для линейной регрессии с настраиваемой жесткостью отбора (в качестве гиперпараметра, зависящего от «Omega»);
3. На отобранных свойствах обучается линейная регрессия с выбором гиперпараметра alpha на Grid Search с кросс-валидацией. Для снижения склонности модели к переобучению, параметр alpha увеличивается на величину, зависящую от гиперпараметра «Omega».   
4. Feature selection для GBMT выполняется по следующему алгоритму:
   - На предварительно отобранных признаках обучается Random forest с гиперпараметрами, обеспечивающими быстрое обучение;
   - Из обученной модели встроенным методом извлекаются оценки значимости признаков, наименее значимые признаки отсекаются по порогу, который зависит от «Omega»;
   - Выполняется обучение модели GBMT на признаках, отобранных на предыдущем шаге;
   - Последовательно на вход модели вместо каждого свойства подается Гауссовский шум, значимость свойств определяется по снижению метрики качества.
   - Производится выбор свойств, вклад которых в качество модели больше порога (порог настраивается как параметр, зависящий от «Omega»);
5. На остатках линейной регрессии производится обучение GBMT, гиперпараметры которого зависят от степени ужесточения регуляризации - гиперпараметра «Omega». Кроме того, при фиксированном «Omega» выполняется небольшой Grid Search для подбора параметров CatBoost. Для остановки обучения используется встроенный «Детектор переобучения».   
6. Пункты 1..5 повторяются многократно с time-split валидацией, выбирается гиперпараметр «Omega», обеспечивающий наилучшее качество. 
   Необходимость выполнения более жесткого отбора признаков и усиления регуляризации продиктованы склонностью ансамбля моделей к переобучению вследствие:
   - Отбора признаков из огромного количества сгенерированных, что требует введения поправок на множественную проверку гипотез.
   - Использования бустинга для ансамблирования двух моделей.  
7. После предварительного расчета гиперпараметров общей модели (по формулам зависимости их от omega), выполняется тонкая настройка каждого гиперпараметра в отдельности.



##### Зависимости:

- Numpy
- Pandas
- Scikit-learn
- CatBoost



##### Требования к данным:

1. Target (y) должен быть представлен объектом pandas series.  
2. Data (X) должна представлена объектом pandas dataframe.
3. X и y должны иметь общий индекс строк.
4. Если в данных есть время и дата, они должны быть индексом как для X, так и для y.



##### Рекомендации по назначению   review_period и forecast_horizon:

**review_period** — это условное количество шагов, на которое алгоритм смотрит «назад» в данные (то есть Time-lag фичи с периодом больше review_period сгенерированы не будут). Этот период нужно выбирать таким, чтобы была учтена сезоннось: обычно, 1, 2 или 3 периода сезонности. Период, равный review_period будет исключен из данных для обучения, поэтому, а также для борьбы с переобучением и нормальной работы модели, общий период наблюдения в данных должен быть не менее чем в 3-4 раза больше review_period.

**forecast_horizon** — это условное количество шагов, которое при обучении берётся со значительно меньшим весом, во избежание сильного влияния утечки данных на переобучение. По умолчанию можно взять равным максимальному планируемому отрезку out of sample прогнозирования, forecast_segment. Можно поставить forecast_horizon немного меньше (в 1,5..2,5 раз), но это может приводить к снижению качества модели вследствие переобучения. По мере уменьшения forecast_horizon при фиксированном  forecast_segment, влияние данных, которые модель сама же сгенерировала на предыдущих шагах, будет увеличиваться. По умолчанию этот параметр равен 1, на случай, если вам необходим прогноз только на один шаг. forecast_horizon должен быть, как минимум, в 2 раза меньше, чем review_period.



##### Пример использования библиотеки:

import TSShaman as ts

import pandas as pd

from sklearn.metrics import r2_score



data = pd.read_csv(‘data.csv’)

y = data[‘target’]

y_train = y[:-365]

y_test = y[-365:]



model = sh.TSShaman(review_period=1095, forecast_horizon=365)

model.fit(y_train)

y_pred = model.predict(forecast_period=365)

print(f’R^2={r2_score(y_test, y_pred):.3f}’)